# -*- coding: utf-8 -*-
"""Dissertation_Unsupervised_Anomaly_Detection_S2596860.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T8ARLbzIxB8J7DsdWR3FH-Oi6WVmRUX4

# ***IMPORTING THE NECESSARY LIBRARIES***
"""

# Importing the drive moduls , giving the appropriate path where I have kept my dataset.

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/Dissertation_Lloyds')

# Importing all the necessary libraries which are later required.

import pandas as pd
from statsmodels.graphics.tsaplots import plot_acf
from scipy.signal import periodogram
import numpy as np
from statsmodels.tsa.seasonal import STL

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA

from sklearn.ensemble import IsolationForest
from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix
from sklearn.svm import OneClassSVM
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed,Input, Dropout
from keras.models import Model
from keras import regularizers

# Loading the data into the 'df' variable.
df = pd.read_csv('Lloyds_data.csv')

# Note to Instructors : Comment this out and try loading the data from your appropriate path onto the 'df' variable for smooth working lateron.

# Printing out the first 10 rows of the dataframe.
df.head(10)

# Visualising the shape of the data.
df.shape

"""# ***PREPROCESSING***"""

# Checking whether any null values are present in our columns.
df.isna().sum()

# Printing out the columns.
df.columns

# Printing out the number of unique values every column has.
df.nunique()

# Finding out the number of records where the spend column value is zero.
df[df['spend'] == 0].shape

df.shape

# Dropping those rows from the dataframe altogether and reseting the indices , giving an updated dataframe.

df.drop(df[df['spend'] == 0].index, inplace=True)
df.reset_index(drop=True, inplace=True)

df

# Printing the new shape of the updated dataframe.
df.shape

# Printing out the distinct days alongside its value count.
df.day_of_week.value_counts()

#  Printing out the unique departments alongside its number of occurences overall in the data.
df.department.value_counts()

"""# ***EXPLORATORY DATA ANALYSIS***

## ***ANALYSING THE TIME SERIES NATURE***
"""

###################################  IMPORTANT POINT TO NOTE ###################################

# The resolution and sizes of the plots including the labels below are scaled up for the same in the report.

# converting the date to datetime format for easy usage later point using appropriate formatting technique.
# This format is used on visualising the date column in the dataframe.

df['date'] = pd.to_datetime(df['date'],format='%d/%m/%Y')

# constructing the pivot table.
pivot_table = pd.pivot_table(df, values='spend', index='date', columns='individual_id', aggfunc=np.sum, fill_value=0)
pivot_table

df.date.value_counts()

# This code performs Seasonal Decomposition of Time Series (STL) on aggregated expenditure data where period is set as 15.
# It groups the data by date and sums the expenditure ('spend'), decomposes the time series into trend, seasonal, and residual components,
# and then plots these components along with the original data for optimal visualisation.

# Getting the aggregated dataframe.
df_agg = df.groupby('date').agg({'spend': 'sum'}).reset_index()

# Perform Seasonal Decomposition (STL)
stl = STL(df_agg['spend'], period = 15)  # seasonal period chosen as 13 for monthly data

res = stl.fit()

# Plot original data, trend, seasonal, and residuals
plt.figure(figsize=(16,9))

# Original data
plt.subplot(4, 1, 1)
plt.plot(df_agg['date'], df_agg['spend'], label='Original Data',color = 'brown')
plt.title('Original Data',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Spend',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Trend component
plt.subplot(4, 1, 2)
plt.plot(df_agg['date'], res.trend, label='Trend', color = 'green')
plt.title('Trend Component',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Trend',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Seasonal component
plt.subplot(4, 1, 3)
plt.plot(df_agg['date'], res.seasonal, label='Seasonal', color = 'blue')
plt.title('Seasonal Component',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Seasonal',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Residual component (noise)
plt.subplot(4, 1, 4)
plt.plot(df_agg['date'], res.resid, label='Residuals' , color = 'red')
plt.title('Residual Component (Noise)',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Residuals',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

plt.tight_layout()
plt.show()

# This code performs Seasonal Decomposition of Time Series (STL) on aggregated expenditure data where period is set as 31.
# It groups the data by date and sums the expenditure ('spend'), decomposes the time series into trend, seasonal, and residual components,
# and then plots these components along with the original data for optimal visualisation.

# Fetching the aggregated dataframe.
df_agg = df.groupby('date').agg({'spend': 'sum'}).reset_index()

# Perform Seasonal Decomposition (STL)
stl = STL(df_agg['spend'], period = 31)  # seasonal period chosen as 13 for monthly data

res = stl.fit()

# Plot original data, trend, seasonal, and residuals
plt.figure(figsize=(16,9), dpi = 300)

# Original data
plt.subplot(4, 1, 1)
plt.plot(df_agg['date'], df_agg['spend'], label='Original Data', color = 'brown')
plt.title('Original Data',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Spend',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Trend component
plt.subplot(4, 1, 2)
plt.plot(df_agg['date'], res.trend, label='Trend', color = 'green')
plt.title('Trend Component',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Trend',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Seasonal component
plt.subplot(4, 1, 3)
plt.plot(df_agg['date'], res.seasonal, label='Seasonal', color = 'blue')
plt.title('Seasonal Component',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Seasonal',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Residual component (noise)
plt.subplot(4, 1, 4)
plt.plot(df_agg['date'], res.resid, label='Residuals', color = 'red')
plt.title('Residual Component (Noise)',fontsize=16)
plt.xlabel('Date',fontsize=16)
plt.ylabel('Residuals',fontsize=16)
plt.legend()
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

plt.tight_layout()
plt.show()

# This code performs Seasonal Decomposition of Time Series (STL) on the mean expenditure ('spend') data over the period = 15.
# It groups the data by date and calculates the mean expenditure, decomposes the time series into trend, seasonal, and residual components,
# and then plots these components along with the original data for better visualization and analysis.

# Fetching the appropriate aggregated dataframe.
df_agg = df.groupby('date').agg({'spend': 'mean'}).reset_index()

# Perform Seasonal Decomposition (STL)
stl = STL(df_agg['spend'], period = 15)  # seasonal period chosen as 13 for monthly data

res = stl.fit()

# Plot original data, trend, seasonal, and residuals
plt.figure(figsize=(14, 10))

# Original data
plt.subplot(4, 1, 1)
plt.plot(df_agg['date'], df_agg['spend'], label='Original Data', color = 'brown')
plt.title('Original Data')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

# Trend component
plt.subplot(4, 1, 2)
plt.plot(df_agg['date'], res.trend, label='Trend', color ='green')
plt.title('Trend Component')
plt.xlabel('Date')
plt.ylabel('Trend')
plt.legend()

# Seasonal component
plt.subplot(4, 1, 3)
plt.plot(df_agg['date'], res.seasonal, label='Seasonal', color = 'blue')
plt.title('Seasonal Component')
plt.xlabel('Date')
plt.ylabel('Seasonal')
plt.legend()

# Residual component (noise)
plt.subplot(4, 1, 4)
plt.plot(df_agg['date'], res.resid, label='Residuals', color = 'red')
plt.title('Residual Component (Noise)')
plt.xlabel('Date')
plt.ylabel('Residuals')
plt.legend()

plt.tight_layout()
plt.show()

# This code performs Seasonal Decomposition of Time Series (STL) on the mean expenditure ('spend') data over the period = 31.
# It groups the data by date and calculates the mean expenditure, decomposes the time series into trend, seasonal, and residual components,
# and then plots these components along with the original data for better visualization and analysis.

# Fetching the appropriate aggregated dataframe based on the desired metric.
df_agg = df.groupby('date').agg({'spend': 'mean'}).reset_index()

# Perform Seasonal Decomposition (STL)
stl = STL(df_agg['spend'], period = 31)  # seasonal period chosen as 13 for monthly data

res = stl.fit()

# Plot original data, trend, seasonal, and residuals
plt.figure(figsize=(14, 10))

# Original data
plt.subplot(4, 1, 1)
plt.plot(df_agg['date'], df_agg['spend'], label='Original Data', color = 'brown')
plt.title('Original Data')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

# Trend component
plt.subplot(4, 1, 2)
plt.plot(df_agg['date'], res.trend, label='Trend', color ='green')
plt.title('Trend Component')
plt.xlabel('Date')
plt.ylabel('Trend')
plt.legend()

# Seasonal component
plt.subplot(4, 1, 3)
plt.plot(df_agg['date'], res.seasonal, label='Seasonal', color = 'blue')
plt.title('Seasonal Component')
plt.xlabel('Date')
plt.ylabel('Seasonal')
plt.legend()

# Residual component (noise)
plt.subplot(4, 1, 4)
plt.plot(df_agg['date'], res.resid, label='Residuals', color = 'red')
plt.title('Residual Component (Noise)')
plt.xlabel('Date')
plt.ylabel('Residuals')
plt.legend()

plt.tight_layout()
plt.show()

"""## ***ANALYSING THE 'SPEND' COLUMN***"""

# Fetching the minimum and maximum of 'spend' column
df.spend.min(),df.spend.max()

df.spend

# Histogram plot to understand the distribution of 'spend' feature column.

#Figure Size
plt.figure(figsize=(12, 9), dpi=300)
sns.histplot(df['spend'], bins=30, kde=True)
plt.title('Histogram Plot of Spend Column', fontsize=16) # Adding a appropriate title.
plt.xlabel('Spend', fontsize=16) # Labeling the X axis.
plt.ylabel('Frequency', fontsize=16) # Labeling the Y axis.
plt.xticks(fontsize=14) # Setting the size of the X axis values.
plt.yticks(fontsize=14) # Setting the size of the Y axis values.
plt.tight_layout()  # Adjust layout to prevent clipping

# Scaling the spend feature column.
scaler = StandardScaler()
df['scaled_spend'] = scaler.fit_transform(df[['spend']])

# Histogram plot in observing the nature of the scaled spend.

plt.figure(figsize=(10, 6))
sns.histplot(df['scaled_spend'], bins=30, kde=True)
plt.title(f'Histogram Plot in understanding the Expenditure data ( Post normalization)')
plt.xlabel('Expenditure')
plt.ylabel('Frequency')
plt.show()

df.scaled_spend.min(),df.scaled_spend.max()

df['spend'].describe() , df['scaled_spend'].describe()

#######.      INSIGHT MADE ########################

# log transformation --- negative values , and scaling does not help in any sense either... thus it is better to keep it as such...

# since it is a time series data , it is improper to perform any kind of transformations , thus , spend as such would be proper ...

# Visualising the box plot of expenditure.

plt.figure(figsize=(8, 6))
plt.boxplot(df['spend'])
plt.title('Box Plot of Expenditure')
plt.xlabel('Spend')
plt.show()

df.columns

df.info()

"""### ***INDIVIDUAL WISE ANALYSIS***

***TOP 10 CONTRIBUTING INDIVIDUALS (BASED ON NUMBER OF TRANSACTIONS)***
"""

df.individual_id.value_counts()

df.individual_id.min() , df.individual_id.max()

dictie = dict(df.individual_id.value_counts())

# Sort the dictionary by values and get top 10 individuals
top_10_individuals = {k:v for k, v in sorted(dictie.items(), key=lambda item: item[1], reverse=True)[:10]}

top_10 = list(top_10_individuals.keys())

top_10

df_one = df[df['individual_id'] == 213]
df_two = df[df['individual_id'] == 1985]
df_three = df[df['individual_id'] == 2182]

df_four = df[df['individual_id'] == 1358]
df_five = df[df['individual_id'] == 561]
df_six = df[df['individual_id'] == 2120]

df_three[df_three['at_risk_event'] == True]

plt.figure(figsize=(12,9),dpi = 500)

plt.plot(df_one['date'], df_one['spend'], label='Individual 213 Spend', color='blue')
plt.scatter(df_one[df_one['at_risk_event']]['date'], df_one[df_one['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 213', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.ylabel('Spend', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(12,9),dpi = 300)

# Individual 1
plt.subplot(3, 1, 1)
plt.plot(df_one['date'], df_one['spend'], label='Individual 213 Spend', color='blue')
plt.scatter(df_one[df_one['at_risk_event']]['date'], df_one[df_one['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 213', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.ylabel('Spend', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend()

# Individual 2
plt.subplot(3, 1, 2)
plt.plot(df_two['date'], df_two['spend'], label='Individual 314 Spend', color='green')
plt.scatter(df_two[df_two['at_risk_event']]['date'], df_two[df_two['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 1985', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.ylabel('Spend', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend()

# Individual 3
plt.subplot(3, 1, 3)
plt.plot(df_three['date'], df_three['spend'], label='Individual 314 Spend', color='orange')
plt.scatter(df_three[df_three['at_risk_event']]['date'], df_three[df_three['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 2182', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.ylabel('Spend', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 10))

# Individual 4
plt.subplot(3, 1, 1)
plt.plot(df_four['date'], df_four['spend'], label='Individual 1358 Spend', color='blue')
plt.scatter(df_four[df_four['at_risk_event']]['date'], df_four[df_four['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 1358')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

# Individual 5
plt.subplot(3, 1, 2)
plt.plot(df_five['date'], df_five['spend'], label='Individual 561 Spend', color='green')
plt.scatter(df_five[df_five['at_risk_event']]['date'], df_five[df_five['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 561')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

# Individual 6
plt.subplot(3, 1, 3)
plt.plot(df_six['date'], df_six['spend'], label='Individual 2120 Spend', color='orange')
plt.scatter(df_six[df_six['at_risk_event']]['date'], df_six[df_six['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 2120')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

plt.tight_layout()
plt.show()

"""***TOP 10 CONTRIBUTING INDIVIDUALS (BASED ON EXPENDITURE)***"""

individual_spend = df.groupby('individual_id')['spend'].sum()
top_10_individuals = individual_spend.sort_values(ascending=False).head(10)

print(top_10_individuals)

spend_one = df[df['individual_id'] == 951]
spend_two = df[df['individual_id'] == 847]
spend_three = df[df['individual_id'] == 1115]

spend_four = df[df['individual_id'] == 1451]
spend_five = df[df['individual_id'] == 974]
spend_six = df[df['individual_id'] == 733]

plt.figure(figsize=(12, 9), dpi = 300)

# Individual 1
plt.subplot(3, 1, 1)
plt.plot(spend_one['date'], spend_one['spend'], label='Individual 951 Spend', color='darkblue')
plt.scatter(spend_one[spend_one['at_risk_event']]['date'], spend_one[spend_one['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 951', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.ylabel('Spend', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend()

# Individual 2
plt.subplot(3, 1, 2)
plt.plot(spend_two['date'], spend_two['spend'], label='Individual 847 Spend', color='darkgreen')
plt.scatter(spend_two[spend_two['at_risk_event']]['date'], spend_two[spend_two['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 847', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.ylabel('Spend', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend()

# Individual 3
plt.subplot(3, 1, 3)
plt.plot(spend_three['date'], spend_three['spend'], label='Individual 1115 Spend', color='darkred')
plt.scatter(spend_three[spend_three['at_risk_event']]['date'], spend_three[spend_three['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 1115', fontsize=16)
plt.xlabel('Date', fontsize=16)
plt.ylabel('Spend', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 10))

# Individual 4
plt.subplot(3, 1, 1)
plt.plot(spend_four['date'], spend_four['spend'], label='Individual 951 Spend', color='darkblue')
plt.scatter(spend_four[spend_four['at_risk_event']]['date'], spend_four[spend_four['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 1451')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

# Individual 5
plt.subplot(3, 1, 2)
plt.plot(spend_five['date'], spend_five['spend'], label='Individual 847 Spend', color='darkgreen')
plt.scatter(spend_five[spend_five['at_risk_event']]['date'], spend_five[spend_five['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 974')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

# Individual 6
plt.subplot(3, 1, 3)
plt.plot(spend_six['date'], spend_six['spend'], label='Individual 1115 Spend', color='cyan')
plt.scatter(spend_six[spend_six['at_risk_event']]['date'], spend_six[spend_six['at_risk_event']]['spend'], color='red', label='At Risk Event')
plt.title('Time Series Analysis for Individual 733')
plt.xlabel('Date')
plt.ylabel('Spend')
plt.legend()

plt.tight_layout()
plt.show()

df.at_risk_event.value_counts()

'''

-> The above time series plots are plotted for the top 6 individuals (based on the number of transactions and then on expenditure)..

-> Line plot seem to be fine and we could properly conclude on the fact that there is no periodicity factor out here..

-> every line plot when plotted for every invidiual over the time seems irregular..

-> Expenditure plot is plotted and a general trend is observed on the top performing individuals..

-> Later point , when the risk events are plotted on the same graphs , we could deduce something interesting..

-> No general conclusion could be made that , the higher expense (spend) would correspond to a risk event...

-> It might be a factor (i.e) There is a high probability that the transaction could be a risk event , but not a sure one...

-> because we could see some events to be modelled as risk events even when the expense is on an average scale or sometimes even lower than the average..

'''

"""## ***ANALYSING DEPARTMENT***"""

df.columns # Printing the columns of the dataframe.

df.department.unique() # Printing out the names of the unique departments present.

# Bar plot on visualising the number of aggregated transactions with respect to every department over the time period present.

# Fetching the counts of every department present.
dept_counts = df['department'].value_counts()
plt.figure(figsize=(12,9), dpi = 300) # Setting up the size of the bar plot.
sns.barplot(x=dept_counts.values, y=dept_counts.index, palette='viridis')
plt.title('Frequency of Transactions made with respect to every department', fontsize = 16) # Setting the title
plt.xlabel('Frequency of Transactions',fontsize = 16) # labeling the X axis.
plt.ylabel('Department',fontsize = 16) # Labeling the Y axis.
plt.xticks(fontsize=14) # Describing the size of the X axis numeric values.
plt.yticks(fontsize=14) # Describing the size of the Y axis numeric values.
plt.show()

# Aggregating the department counts and the number of associated risk events , thereby calculating the risk ratio as
# (Number of risk events / number of total transactions aggregated over the department).

department_counts = df.groupby('department')['at_risk_event'].count().reset_index(name='total_events')
risk_events = df.groupby('department')['at_risk_event'].sum().reset_index(name='risk_events')
department_counts = pd.merge(department_counts, risk_events, on='department')

# Calculating the risk ratio.
department_counts['risk_ratio'] = department_counts['risk_events'] / department_counts['total_events']

# Risk Ratio vs Department Line graph is plotted.
plt.figure(figsize=(15,10))
plt.plot(department_counts['department'], department_counts['risk_ratio'], marker='o', linestyle='-', color='b', label='Risk Ratio')
plt.title('Risk Ratio per Department')
plt.xlabel('Department')
plt.ylabel('Risk Ratio')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.show()

department_counts # Printing the department_counts variable.

# Visualising the aggregated spend and risk ratio for every department present.

department_counts = df.groupby('department')['at_risk_event'].count().reset_index(name='total_events')
risk_events = df.groupby('department')['at_risk_event'].sum().reset_index(name='risk_events')
department_counts = pd.merge(department_counts, risk_events, on='department')

# Calculate risk ratio
department_counts['risk_ratio'] = department_counts['risk_events'] / department_counts['total_events']

# Calculate aggregated spend per department
department_spend = df.groupby('department')['spend'].sum().reset_index(name='total_spend')

# Merge spend data with department counts
department_counts = pd.merge(department_counts, department_spend, on='department')

# Plotting
fig, ax1 = plt.subplots(figsize=(16, 9), dpi=300)

# Plotting risk ratio
ax1.plot(department_counts['department'], department_counts['risk_ratio'], marker='o', linestyle='-', color='b', label='Risk Ratio')
ax1.set_xlabel('Department', fontsize=16)
ax1.set_ylabel('Risk Ratio', color='black', fontsize=16)
ax1.tick_params(axis='x', labelsize=14)
ax1.tick_params(axis='y', labelcolor='black', labelsize=14)
ax1.grid(True)

# Creating a second y-axis for aggregated spend
ax2 = ax1.twinx()
ax2.plot(department_counts['department'], department_counts['total_spend'], marker='s', linestyle='-', color='r', label='Aggregated Spend')
ax2.set_ylabel('Aggregated Spend', color='black', fontsize=16)
ax2.tick_params(axis='y', labelcolor='black', labelsize=14)

plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')

# Title and legend
plt.title('Risk Ratio and Aggregated Spend per Department', fontsize=16)
fig.tight_layout()
fig.legend(loc='upper right')

plt.show()

# Visualising the risk ratio and average spend for every department present.

# Calculate total events and risk events per department
department_counts = df.groupby('department')['at_risk_event'].count().reset_index(name='total_events')
risk_events = df.groupby('department')['at_risk_event'].sum().reset_index(name='risk_events')
department_counts = pd.merge(department_counts, risk_events, on='department')

# Calculate risk ratio
department_counts['risk_ratio'] = department_counts['risk_events'] / department_counts['total_events']

# Calculate mean spend per department
department_mean_spend = df.groupby('department')['spend'].mean().reset_index(name='mean_spend')

# Plotting
fig, ax1 = plt.subplots(figsize=(16,9), dpi = 300)

# Plotting risk ratio
ax1.plot(department_counts['department'], department_counts['risk_ratio'], marker='o', linestyle='-', color='b', label='Risk Ratio')
ax1.set_xlabel('Department',fontsize = 16)
ax1.set_ylabel('Risk Ratio', color='black',fontsize = 16)
ax1.tick_params(axis='x', labelsize=14)
ax1.tick_params(axis='y', labelcolor='black', labelsize=14)
ax1.grid(True)

# Creating a second y-axis for mean spend
ax2 = ax1.twinx()
ax2.plot(department_mean_spend['department'], department_mean_spend['mean_spend'], marker='s', linestyle='-', color='g', label='Mean Spend')
ax2.set_ylabel('Mean Spend', color='black',fontsize = 16)
ax2.tick_params(axis='y', labelcolor='black', labelsize=14)

# Title and legend
plt.title('Risk Ratio and Mean Spend per Department',fontsize = 16)
fig.tight_layout()
fig.legend(loc='upper right')


plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')
# Rotate and adjust x-axis labels
plt.xticks(rotation=45, ha='right')  # Rotate labels and align them to the right

plt.show()

# Box plot of spending nature of every department is observed.

plt.figure(figsize=(10, 6))
sns.boxplot(x='department', y='spend', data=df)
plt.title('Spend by Department')
plt.xlabel('Department')
plt.ylabel('Spend')
plt.xticks(rotation=45)
plt.show()

# Aggregated spending nature is fetched here for every department present.

department_tot_spend = df.groupby('department')['spend'].sum().sort_values(ascending = False).reset_index()
department_tot_spend.head(27)

# Plotting the total spend for every department present.

plt.figure(figsize=(15, 8))
plt.plot(department_tot_spend['department'], department_tot_spend['spend'], marker='o', linestyle='-')
plt.title('Total Spend by Department')
plt.xlabel('Department')
plt.ylabel('Total Spend')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Average spending nature is fetched here for every department present.
department_avg_spend = df.groupby('department')['spend'].mean().sort_values(ascending = False).reset_index()
department_avg_spend.head(27)

# Plotting the average spend for every department present.
plt.figure(figsize=(15, 8))
plt.plot(department_avg_spend['department'], department_avg_spend['spend'], marker='o', linestyle='-')
plt.title('Average Spend by Department')
plt.xlabel('Department')
plt.ylabel('Average Spend')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""## ***TIMELINE TREND***

### ***DAY WISE ANALYSIS***
"""

# Printing out the unique days available in the datframe.
df.day_of_week.unique()

# Printing out the unique days available alongside its total count (number of occuerences over the entire data).
day_counts = df['day_of_week'].value_counts()
day_counts

# Setting the day order explicitly for easier visualisation later in the bar plot.
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

# Total transactions made aggregated over the available days throughout the entire time period.

plt.figure(figsize=(12, 9), dpi=300)
sns.barplot(x=day_counts.index, y=day_counts.values, order=day_order, palette='viridis')
plt.title('Frequency of Transactions Made with Respect to Every Day', fontsize=16)
plt.xlabel('Available Days of the Week', fontsize=16)
plt.ylabel('Frequency', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.tight_layout()
plt.show()

df[(df['day_of_week'] == 'Friday') & (df['at_risk_event'] == True)].shape

day_counts

# This code calculates the risk ratio per day of the week and we therefore visualise through a plot for our convenience.
# It groups the data by the day of the week, calculates the total and at-risk events,
# computes the risk ratio, and then plots the risk ratio for each day of the week.

# Calculate total events and risk events per day of the week
day_counts = df.groupby('day_of_week')['at_risk_event'].count().reset_index(name='total_day_events')
risk_day_events = df.groupby('day_of_week')['at_risk_event'].sum().reset_index(name='total_risk_events')
day_counts = pd.merge(day_counts, risk_day_events, on='day_of_week')

# Calculate risk ratio
day_counts['risk_ratio'] = day_counts['total_risk_events'] / day_counts['total_day_events']


# Setting up the order for easy visualisation.
day_counts['day_of_week'] = pd.Categorical(day_counts['day_of_week'], categories=day_order, ordered=True)
day_counts = day_counts.sort_values('day_of_week')

# Plotting
plt.figure(figsize=(15,10))
plt.plot(day_counts['day_of_week'], day_counts['risk_ratio'], marker='o', linestyle='-', color='b', label='Risk Ratio')
plt.title('Risk Ratio per Day')
plt.xlabel('Day of the Week')
plt.ylabel('Risk Ratio')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.show()

# Visualising the aggregated spend and risk ratio for every day present.


day_counts = df.groupby('day_of_week')['at_risk_event'].count().reset_index(name='total_day_events')
risk_day_events = df.groupby('day_of_week')['at_risk_event'].sum().reset_index(name='total_risk_events')
day_counts = pd.merge(day_counts, risk_day_events, on='day_of_week')

day_counts['risk_ratio'] = day_counts['total_risk_events'] / day_counts['total_day_events']

# Calculate total spend per department
day_total_spend = df.groupby('day_of_week')['spend'].sum().reset_index(name='total_spend')

day_counts = day_counts.set_index('day_of_week').loc[day_order].reset_index()
day_total_spend = day_total_spend.set_index('day_of_week').loc[day_order].reset_index()

# Plotting
fig, ax1 = plt.subplots(figsize=(12, 9), dpi=300)

# Plotting risk ratio
ax1.plot(day_counts['day_of_week'], day_counts['risk_ratio'], marker='o', linestyle='-', color='b', label='Risk Ratio')
ax1.set_xlabel('Day of Week', fontsize=20)
ax1.set_ylabel('Risk Ratio', color='black', fontsize=20)
ax1.tick_params(axis='y', labelcolor='black', labelsize=18)
ax1.tick_params(axis='x', labelsize=18)
ax1.grid(True)

# Creating a second y-axis for total spend
ax2 = ax1.twinx()
ax2.plot(day_total_spend['day_of_week'], day_total_spend['total_spend'], marker='s', linestyle='-', color='r', label='Total Spend')
ax2.set_ylabel('Aggregated Spend', color='black', fontsize=20)
ax2.tick_params(axis='y', labelcolor='black', labelsize=14)

# Title and legend
plt.title('Risk Ratio and Aggregated Spend on a Daily Basis', fontsize=18)
fig.tight_layout()
fig.legend(loc='upper right',fontsize=20)

# Rotate and adjust x-axis labels
plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')

plt.show()

# Visualising the mean spend and risk ratio for every day present.

# Grouping and calculating required metrics
day_counts = df.groupby('day_of_week')['at_risk_event'].count().reset_index(name='total_day_events')
risk_day_events = df.groupby('day_of_week')['at_risk_event'].sum().reset_index(name='total_risk_events')
day_counts = pd.merge(day_counts, risk_day_events, on='day_of_week')

day_counts['risk_ratio'] = day_counts['total_risk_events'] / day_counts['total_day_events']

# Calculate mean spend per department
day_mean_spend = df.groupby('day_of_week')['spend'].mean().reset_index(name='mean_spend')

day_counts = day_counts.set_index('day_of_week').loc[day_order].reset_index()
day_mean_spend = day_mean_spend.set_index('day_of_week').loc[day_order].reset_index()

# Plotting
fig, ax1 = plt.subplots(figsize=(12, 9), dpi=300)

# Plotting risk ratio
ax1.plot(day_counts['day_of_week'], day_counts['risk_ratio'], marker='o', linestyle='-', color='b', label='Risk Ratio')
ax1.set_xlabel('Day of Week', fontsize=16)
ax1.set_ylabel('Risk Ratio', color='black', fontsize=16)
ax1.tick_params(axis='y', labelcolor='black', labelsize=14)
ax1.tick_params(axis='x', labelsize=14)
ax1.grid(True)

# Creating a second y-axis for mean spend
ax2 = ax1.twinx()
ax2.plot(day_mean_spend['day_of_week'], day_mean_spend['mean_spend'], marker='s', linestyle='-', color='green', label='Mean Spend')
ax2.set_ylabel('Mean Spend', color='black', fontsize=16)
ax2.tick_params(axis='y', labelcolor='black', labelsize=14)

# Title and legend
plt.title('Risk Ratio and Mean Spend on a Daily Basis', fontsize=16)
fig.tight_layout()
fig.legend(loc='upper right')

# Rotate and adjust x-axis labels
plt.xticks(rotation=45, ha='right')  # Rotate labels and align them to the right

plt.show()

# Box plot visualisation for every day of the week present.

plt.figure(figsize=(10, 6))
sns.boxplot(x='day_of_week', y='spend', data=df)
plt.title('Spend by Day of Week')
plt.xlabel('Day of Week')
plt.ylabel('Spend')
plt.show()

"""### ***MONTHLY ANALYSIS***"""

df.columns

# Fetching the appropriate month from the date column (which is already in datetime format).
df['month'] = df['date'].dt.month
df[['date','month']]

# Printing out the month and its value counts.
df.month.value_counts()

# Mapping every month with its appropriate name.

month_names = {
    1: 'January', 2: 'February', 3: 'March', 4: 'April',
    5: 'May', 6: 'June', 7: 'July', 8: 'August',
    9: 'September', 10: 'October', 11: 'November', 12: 'December'
}

df['month_name'] = df['month'].map(month_names)

# Setting the order of the months.
month_order = ['January', 'February', 'March', 'April']

'''

Older one

month_counts = df['month_name'].value_counts()
month_counts

plt.figure(figsize=(10, 8))
sns.barplot(x=month_counts.index, y= month_counts.values,order = month_order, palette='viridis')
plt.title('Frequency of Transactions made with respect to every month present',fontsize = 12)
plt.xlabel('Available Months',fontsize = 12)
plt.ylabel('Frequency of transactions',fontsize = 12)
plt.show()

'''

# Total number of transactions aggregated over every month across the entire time period present.

month_counts = df['month_name'].value_counts()
month_counts

plt.figure(figsize=(12,9), dpi = 300)
sns.barplot(x=month_counts.index, y= month_counts.values,order = month_order, palette='viridis')
plt.title('Frequency of Transactions made with respect to every month present',fontsize = 16)
plt.xlabel('Available Months',fontsize = 16)
plt.ylabel('Frequency of transactions',fontsize = 16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()

# Plotting the risk ratio for every month present.

month_counts = df.groupby('month_name')['at_risk_event'].count().reset_index(name='total_month_events')
risk_month_events = df.groupby('month_name')['at_risk_event'].sum().reset_index(name='total_risk_events')
month_counts = pd.merge(month_counts, risk_month_events, on='month_name')

month_counts['risk_ratio'] = month_counts['total_risk_events'] / month_counts['total_month_events']

month_counts = month_counts.set_index('month_name').loc[month_order].reset_index()

# Plotting
plt.figure(figsize=(12,9))
plt.plot(month_counts['month_name'], month_counts['risk_ratio'], marker='o', linestyle='-', color='blue', label='Risk Ratio')
plt.title('Risk Ratio on a monthly basis',fontsize = 16)
plt.xlabel('Available Month',fontsize = 16)
plt.ylabel('Risk Ratio',fontsize = 16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.grid(True)
plt.legend()
plt.show()

"""### ***HOURLY ANALYSIS***"""

df.columns

df.shape

df.hour_of_day.nunique()

# Fetching the new column which has the hour in whole number format.

df['rounded_hour'] = df['hour_of_day'].round().astype('int')
df[['hour_of_day','rounded_hour']]

df.rounded_hour.min(), df.rounded_hour.max()

df.rounded_hour.nunique()

hourly_transaction_counts = df['rounded_hour'].value_counts().sort_index()
hourly_transaction_counts

# Plotting the aggregated transactions over every hour of the day present.

plt.figure(figsize=(12, 9), dpi=300)
hourly_transaction_counts.plot(kind='bar', color='blue')
plt.title('Frequency of Transactions by Hour', fontsize=16)
plt.xlabel('Hour of the Day', fontsize=16)
plt.ylabel('Number of Transactions', fontsize=16)
plt.xticks(rotation=0, fontsize=14)
plt.yticks(fontsize=14)
plt.tight_layout()

# Plotting the risk ratio present for every available hour present.

hour_counts = df.groupby('rounded_hour')['at_risk_event'].count().reset_index(name='total_hour_events')
risk_hour_events = df.groupby('rounded_hour')['at_risk_event'].sum().reset_index(name='total_risk_events')
hour_counts = pd.merge(hour_counts, risk_hour_events, on='rounded_hour')

hour_counts['risk_ratio'] = hour_counts['total_risk_events'] / hour_counts['total_hour_events']


# Plotting the graph.
plt.figure(figsize=(12,9))
plt.plot(hour_counts['rounded_hour'], hour_counts['risk_ratio'], marker='o', linestyle='-', color='blue', label='Risk Ratio')
plt.title('Risk Ratio per every available hour',fontsize =20)
plt.xlabel('Hour of the Day',fontsize = 20)
plt.ylabel('Risk Ratio',fontsize = 20)
plt.xticks(rotation=0, fontsize= 20)
plt.yticks(fontsize=20)
plt.grid(True)
plt.legend(fontsize = 20)
plt.show()

####   SOME INSIGHTS ON THE ABOVE DATETIME ANALYSIS ##########

# Number of transactions are lesser on the weekend as compared to the four available weekdays...

# isk events are observed to be more probable on the weekend (Fri + Sat + Sun) as compared to the number on the weekdays.

# The highest risk events are observed on Saturday followed by Sunday.

# Extreme outliers are observed on every day of the week present..

# Hours are rounded off , where it is now a 24 hour format (0,1,2,3.....24)

# Let us consider the working hours to be 8.00 AM - 6.00 PM ( 8.00 hrs - 18.00 hrs)...

# As per the above inference, we could say a lot of transactions happen between the working hours and hardly anything on both of the extremeties.

# Starts to gradually increase from the 8th hour, peaks at 13th hour and then a gradual decrease after it until the 18th hour...

# Risk events, as expected occur more on the out of office hours.... ( [1 - 4] and [20 - 23]).

# But on the contrary, we are expecting a rise of risk events in ther working hours as well , from the 8th hour to the 17th hour... (interesting!).

df.columns

"""## ***CORRELATION AND COVARIANCE CALCULATION***"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

df.columns

corr_df = df[['day_of_week','hour_of_day','department','spend','month','at_risk_event']]
corr_df

corr_df['department'] = label_encoder.fit_transform(corr_df['department'])
corr_df['day_of_week'] = label_encoder.fit_transform(corr_df['day_of_week'])

corr_df.cov() # Describing the covariance measures across every numerical column present.

corr_df.corr() # Describing the correlation scores among the features present.

corr_df.corr()['at_risk_event'] # Seeing the correlation scores with respect to the target feature (at_risk_event).

# Plotting the Pearson Correlation Heatmap.

plt.figure(figsize=(12, 9), dpi=300)
correlation_matrix = corr_df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"size": 14})
plt.title('Pearson Correlation Heatmap', fontsize=16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.tight_layout()  # Adjust layout to prevent clipping

plt.show()

"""# ***FEATURE ENGINEERING***"""

df.columns

df.shape

"""## ***AGGREGATE LEVEL FEATURES***"""

## Aggregate features are found out below on spend amount grouped by 'individuals' present..

## for instance, mean_amount of individual_id == 1 would take up a single value on every record which captures the overall mean of that individual...

## likewise the calculation on other statistical measure is done and specified here...

# Performs various aggregations on the 'spend' column grouped by 'individual_id' and merges the results back to the original dataframe.
# The aggregations encompass mean, median, sum, standard deviation, minimum, and maximum of the 'spend' column.

# Aggregations are performed
aggregations = df.groupby('individual_id')['spend'].agg(['mean', 'median', 'sum', 'std', 'min', 'max']).reset_index()

#Renaming the columns for a better understanding.
aggregations.columns = ['individual_id', 'mean_amount', 'median_amount', 'total_amount', 'std_amount', 'min_amount', 'max_amount']

# Merging onto the main dataframe.
df = df.merge(aggregations, on='individual_id', how='left')

# This code performs daily aggregations on the 'spend' column grouped by 'individual_id' and 'date',
# and then merges the results back to the original dataframe.
# The aggregations include mean, sum, and standard deviation of the 'spend' column for each day.

daily_aggregations = df.groupby(['individual_id', 'date'])['spend'].agg(['mean', 'sum', 'std']).reset_index()
daily_aggregations.columns = ['individual_id', 'date', 'daily_mean_amount', 'daily_total_amount', 'daily_std_amount']
df = df.merge(daily_aggregations, on=['individual_id', 'date'], how='left')

df[df['individual_id'] == 1][['date','spend','daily_total_amount','daily_mean_amount']].head(20)

"""## ***ROLLING WINDOW FEATURES***"""

# Calculate rolling features over a 7-day window

df['rolling_mean_amount_7d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())
df['rolling_std_amount_7d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=7, min_periods=1).std())
df['rolling_sum_amount_7d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=7, min_periods=1).sum())
df['rolling_transaction_count_7d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=7, min_periods=1).count())

# Calculate rolling features over a 5-day window

df['rolling_mean_amount_5d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())
df['rolling_std_amount_5d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=5, min_periods=1).std())
df['rolling_sum_amount_5d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=5, min_periods=1).sum())
df['rolling_transaction_count_5d'] = df.groupby('individual_id')['spend'].transform(lambda x: x.rolling(window=5, min_periods=1).count())

"""## ***OTHER FEATURES***"""

df[['date']]

df[['date','individual_id']]

# Time since last transaction feature is derived -- where it captures the time difference between two transcations over a particual individual.

df = df.sort_values(by=['individual_id', 'date'])
df['time_since_last_transaction'] = df.groupby('individual_id')['date'].diff().dt.total_seconds().fillna(0) / 3600

df[['date','individual_id','time_since_last_transaction']]

#  The ratio is calculated to understand on how much the individual spend amount is deviating from the average spend.

df['amount_to_mean_ratio'] = df['spend'] / df['mean_amount']

# Calculate cumulative sum of spend
df['cumulative_sum_amount'] = df.groupby('individual_id')['spend'].cumsum()

df['lag_amount_1'] = df.groupby('individual_id')['spend'].shift(1) # Calculate lagged amount (1 day lag)

# Label encoding the department feature column.

label_encoder = LabelEncoder()

df['department_encoded'] = label_encoder.fit_transform(df['department'])

df.day_of_week.value_counts()

day_mapping = {'Monday' : 1, 'Tuesday' : 2, 'Wednesday' : 3, 'Thursday' : 4, 'Friday' : 5, 'Saturday': 6, 'Sunday' : 7}

df['day_of_week_enc'] = df['day_of_week'].map(day_mapping)

df.day_of_week_enc.value_counts()

# Deriving the feature to understand on whether the transaction happens over the weekday or weekend.

df['is_weekend'] = df['day_of_week_enc'].isin([5,6]).astype(int)
df.is_weekend.value_counts()

# Capturing the total spend and mean spend (department wise).

df['spend_per_dept'] = df.groupby('department')['spend'].transform('sum')
df['avg_spend_per_dept'] = df.groupby('department')['spend'].transform('mean')

# Capturing the total spend and mean spend (based on day of week).

df['spend_per_day'] = df.groupby('day_of_week_enc')['spend'].transform('sum')
df['avg_spend_per_day'] = df.groupby('day_of_week_enc')['spend'].transform('mean')

# Capturing the total spend and mean spend (based on every hour of the day present).

df['spend_per_hour'] = df.groupby('rounded_hour')['spend'].transform('sum')
df['avg_spend_per_hour'] = df.groupby('rounded_hour')['spend'].transform('mean')

df[['spend_per_day','avg_spend_per_day','avg_spend_per_hour']]

df.columns

df.shape

df_corr_new = df[['spend', 'at_risk_event',
       'at_risk_behaviour_window', 'rounded_hour',
       'mean_amount', 'median_amount', 'total_amount', 'std_amount',
       'min_amount', 'max_amount', 'daily_mean_amount', 'daily_total_amount',
       'daily_std_amount', 'rolling_mean_amount_7d', 'rolling_std_amount_7d',
       'rolling_sum_amount_7d', 'rolling_transaction_count_7d',
       'rolling_mean_amount_5d', 'rolling_std_amount_5d',
       'rolling_sum_amount_5d', 'rolling_transaction_count_5d',
       'amount_to_mean_ratio','cumulative_sum_amount', 'lag_amount_1', 'day_of_week_enc',
       'is_weekend','spend_per_dept','department_encoded',
       'avg_spend_per_dept', 'spend_per_day', 'avg_spend_per_day']]

df_corr_new.corr()['at_risk_event']

"""# ***TRAIN TEST SPLIT***"""

np.random.seed(42)
unique_individuals = df['individual_id'].unique()

# Randomly select 1748 individuals --- since we calculated and got to know that setting here would get us
# 80% in the train data and 20% in the test data.

selected_individuals = np.random.choice(unique_individuals, size=1742, replace=False)

# Create train and test DataFrames
train_df = df[df['individual_id'].isin(selected_individuals)]
test_df = df[~df['individual_id'].isin(selected_individuals)]

train_df.shape , test_df.shape # Printing out the shapes of train and test dataset.

len(unique_individuals) - len(selected_individuals)

selected_individuals

train_df.shape , test_df.shape

df.columns

# Manaul selection of desired features which could be useful for modeling purpose.
# Some evident features like individual_id and timestamp are ignore....

features = ['hour_of_day', 'spend', 'rounded_hour',
       'mean_amount', 'median_amount', 'total_amount', 'std_amount',
       'min_amount', 'max_amount', 'daily_mean_amount', 'daily_total_amount',
       'daily_std_amount', 'rolling_mean_amount_7d', 'rolling_std_amount_7d',
       'rolling_sum_amount_7d', 'rolling_transaction_count_7d',
       'rolling_mean_amount_5d', 'rolling_std_amount_5d',
       'rolling_sum_amount_5d', 'rolling_transaction_count_5d',
       'time_since_last_transaction', 'amount_to_mean_ratio',
       'cumulative_sum_amount', 'lag_amount_1', 'department_encoded',
       'day_of_week_enc', 'is_weekend', 'spend_per_dept', 'avg_spend_per_dept',
       'spend_per_day', 'avg_spend_per_day']

# Another potential set of useful features are fetched based on domain knowledge.

updated_features = ['spend','rounded_hour','mean_amount', 'median_amount', 'total_amount',
       'min_amount', 'max_amount', 'daily_mean_amount', 'daily_total_amount',
        'rolling_mean_amount_7d',
       'rolling_sum_amount_7d', 'rolling_transaction_count_7d',
       'rolling_mean_amount_5d',
       'rolling_sum_amount_5d', 'rolling_transaction_count_5d', 'amount_to_mean_ratio',
       'cumulative_sum_amount', 'day_of_week_enc',
       'is_weekend','spend_per_dept','department_encoded',
       'avg_spend_per_dept', 'spend_per_day', 'avg_spend_per_day']


base_features = ['hour_of_day','spend','at_risk_event','rounded_hour','day_of_week_enc', 'is_weekend', 'rolling_mean_spending',
       'rolling_mean_spend', 'total_spend', 'avg_spend', 'max_spend',
       'min_spend', 'dev_spend', 'spend_per_dept', 'avg_spend_per_dept',
       'spend_per_day', 'avg_spend_per_day']

# Some derived features like 'daily_std_amount' has NA values , which are imputed with zero for easy modeling.

train_df[features] = train_df[features].fillna(0)
test_df[features] = test_df[features].fillna(0)

train_df[features].isna().sum()

"""# ***FEATURE SELECTION***"""

# Scaling on the train and test dataframe is done which has our desired features through manual feature selection (the list derived in the above section).
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(train_df[features])
X_test_scaled = scaler.fit_transform(test_df[features])

# Apply PCA without specifying n_components to examine explained variance
pca = PCA()
pca.fit(X_train_scaled)

# Calculate cumulative explained variance accordingly.
cumulative_explained_variance = pca.explained_variance_ratio_.cumsum()

# Plot cumulative explained variance
plt.figure(figsize=(10, 6))
plt.plot(cumulative_explained_variance, marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by Number of Principal Components')
plt.grid(True)
plt.show()

# Determine the number of components that explain a desired amount of variance (e.g., 95%)
optimal_n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1

print(f"Optimal number of components to retain 95% variance: {optimal_n_components}")

pca = PCA(n_components = 16)  # Specify the number of components you want
pca.fit(X_train_scaled)

# Get the loadings (coefficients of the original features on the principal components)
loadings = pca.components_.T

# Create a DataFrame for the loadings for better readability
loadings_df = pd.DataFrame(loadings, index=features, columns=[f'PC{i+1}' for i in range(pca.n_components_)])

# Calculate the importance of each feature by summing the absolute values of the loadings across all principal components
feature_importance = loadings_df.abs().sum(axis=1)

# Sort the features by importance
sorted_features = feature_importance.sort_values(ascending=False)

# Changing the number 'N' below explicitly each time fetched us the table in the report
# which has all the results of all the models respectively..

# Each time the appropriate 'N' is set here, and then all the three models are ran after whhich the scores are noted..
# This is repeated for N = 15,20,25 and 31 (refer the table for the scores)..

# Select the top N features where 'N' is set as per our convenience.
top_n_features = sorted_features.head(25)  # Replace the number inside the head accordingly to select that number of features.

print("Top features based on PCA loadings:")
print(top_n_features)

pca_features = top_n_features.index.tolist()

len(pca_features) # Final list of features which we have acquired through the manual 'N' which we have explicitly set.

train_df[pca_features]

"""# ***MODELLING***

## ***CONVENTIONAL APPROACH***

### ***ISOLATION FOREST MODEL***
"""

df.individual_id.nunique()

df.columns

df.individual_id.nunique()

train_df.individual_id.nunique() , test_df.individual_id.nunique()

train_df.shape , test_df.shape

train_df.at_risk_event.value_counts() # Figuring out the number of risk events on the train dataset.

test_df.at_risk_event.value_counts()  # Figuring out the number of risk events on the test datase.

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(train_df[pca_features])
X_test_scaled = scaler.fit_transform(test_df[pca_features])

X_test_scaled

# Fitting the Isolation Forest Model with appropriate hyperparameters.

# The below hyperparameter values are considered to give optimal results which have been got after testing on multiple rounds of iterations.

model = IsolationForest(n_estimators = 10000, contamination=0.20, max_samples = 'auto', random_state=42)
model.fit(X_train_scaled)

# Calculating the anomaly score and figuring out the anomalies , remapping is done on the last step for our convenience in calculation.

test_df['anomaly_score'] = model.decision_function(X_test_scaled)
test_df['anomaly'] = model.predict(X_test_scaled)
test_df['anomaly'] = test_df['anomaly'].map({1: 0, -1: 1})

# Remapping in such a way that now 1 indicates an anomaly whereas 0 is indicative of a normal event.....

# Compute confusion matrix
cm = confusion_matrix(test_df['at_risk_event'], test_df['anomaly'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Risk'])

# Plot confusion matrix
fig, ax = plt.subplots(figsize=(12,9))  # Adjust size if needed
disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=True)

# Customize the numbers inside the confusion matrix
for text in ax.texts:
    text.set_fontsize(30)

# Set title and labels with fontsize
plt.title('Confusion Matrix for Isolation Forest', fontsize=30)
ax.set_xlabel('Predicted Label', fontsize=30)
ax.set_ylabel('True Label', fontsize=30)
ax.tick_params(axis='both', labelsize=30)  # Adjust tick label size if needed

plt.show()

# Precision, Recall and F1 scores are calculated on the test dataset.

precision_test = precision_score(test_df['at_risk_event'], test_df['anomaly'])
recall_test = recall_score(test_df['at_risk_event'], test_df['anomaly'])
f1_test = f1_score(test_df['at_risk_event'], test_df['anomaly'])

# Printing out the fetched scores.

print(f'Test F1 Score: {f1_test}')
print(f'Test Precision: {precision_test}')
print(f'Test Recall: {recall_test}')

# AUC-ROC Score
auc_roc = roc_auc_score(test_df['at_risk_event'], test_df['anomaly_score'])
print(f"AUC-ROC Score: {auc_roc}")

# AUPR Score
precision, recall, _ = precision_recall_curve(test_df['at_risk_event'], test_df['anomaly_score'])
aupr = auc(recall, precision)
print(f"AUPR Score: {aupr}")

"""### ***ONE CLASS SVM***"""

len(pca_features)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(train_df[pca_features])
X_test_scaled = scaler.fit_transform(test_df[pca_features])

# Define the One-Class SVM model
model = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.1)

# Train the model
model.fit(X_train_scaled)         # rbf is good so far..... nu = 0.1 is good as well , gamma = 0.01 gives me a good score

# Calculate anomaly scores
test_df['anomaly_score_oc'] = model.decision_function(X_test_scaled)

# Predict anomalies
test_df['anomaly_oc'] = model.predict(X_test_scaled)

# Map the predictions to a more interpretable format (1 for anomaly, 0 for normal)
test_df['anomaly_oc'] = test_df['anomaly_oc'].map({1: 0, -1: 1})

# Displaying the confusion matrix.

cm = confusion_matrix(test_df['at_risk_event'], test_df['anomaly_oc'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Risk'])


disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for One-Class SVM')
plt.show()

# Precision, Recall and F1 scores are calculated on the test dataset.

precision_test_oc = precision_score(test_df['at_risk_event'], test_df['anomaly_oc'])
recall_test_oc = recall_score(test_df['at_risk_event'], test_df['anomaly_oc'])
f1_test_oc = f1_score(test_df['at_risk_event'], test_df['anomaly_oc'])

# Printing out the fetched scores.

print(f'Test F1 Score: {f1_test_oc}')
print(f'Test Precision: {precision_test_oc}')
print(f'Test Recall: {recall_test_oc}')

# AUC-ROC Score
auc_roc = roc_auc_score(test_df['at_risk_event'], test_df['anomaly_score_oc'])
print(f"AUC-ROC Score: {auc_roc}")

# AUPR Score
precision, recall, _ = precision_recall_curve(test_df['at_risk_event'], test_df['anomaly_score_oc'])
aupr = auc(recall, precision)
print(f"AUPR Score: {aupr}")

"""## ***DEEP LEARNING APPROACH***

### ***LSTM AUTOENCODERS***
"""

len(pca_features) # Fetching the length of pca_features which we have acquired ( just for reaffirmation)...

# Scaling the train and test dataframes.

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(train_df[pca_features])
X_test_scaled = scaler.fit_transform(test_df[pca_features])

# Reshape the data for LSTM
timesteps = 1
X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, X_train_scaled.shape[1]))
X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, X_test_scaled.shape[1]))

X_train_scaled.shape, X_test_scaled.shape

# Defining the autoencoder model.

def autoencoder_model(X):

    model = Sequential([
    LSTM(64, activation='relu', input_shape=(timesteps, X_train_scaled.shape[2]), return_sequences=True),
    LSTM(32, activation='relu', return_sequences=False),
    RepeatVector(timesteps),
    LSTM(32, activation='relu', return_sequences=True),
    LSTM(64, activation='relu', return_sequences=True),
    TimeDistributed(Dense(X_train_scaled.shape[2]))
])
    return model

# Call the model.
model = autoencoder_model(X_train_scaled)

# compile the model.
model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')

# Printing out the summary of the model.
model.summary()

X_train_scaled.shape, train_df[pca_features].shape

# Fitting the model.
nb_epochs = 25
batch_size = 64
history = model.fit(X_train_scaled, X_train_scaled, epochs=nb_epochs, batch_size=batch_size,
                    validation_split=0.05).history

# Predict the values for the training set using the trained model
X_pred = model.predict(X_train_scaled)

# Reshape the predictions to have the same shape as the input features
X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])

# Convert the predictions to a DataFrame and set column names to match the original features
X_pred = pd.DataFrame(X_pred, columns=train_df[pca_features].columns)

# Ensure the index of the predictions matches the index of the original training data
X_pred.index = train_df[pca_features].index

# Create a DataFrame to store the reconstruction error scores
scored = pd.DataFrame(index=train_df[pca_features].index)

# Reshape the original training data to have the same shape as the predictions
Xtrain = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[2])

# Calculate the Mean Absolute Error (MAE) loss for each sample
train_mae_loss = np.mean(np.abs(X_pred-Xtrain), axis = 1)

# Store the MAE loss in the 'scored' DataFrame
scored['Loss_mae'] = train_mae_loss

# Finding out the threshold and storing it with the help of train data loss.
threshold = np.percentile(train_mae_loss, 95)
threshold

# Predict the values for the test set using the trained model
X_pred = model.predict(X_test_scaled)

# Reshape the predictions to have the same shape as the input features
X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])

# Convert the predictions to a DataFrame and set column names to match the original features
X_pred = pd.DataFrame(X_pred, columns=test_df[pca_features].columns)

# Ensure the index of the predictions matches the index of the original test data
X_pred.index = test_df[pca_features].index

# Create a DataFrame to store the reconstruction error scores
scored = pd.DataFrame(index=test_df[pca_features].index)

# Reshape the original test data to have the same shape as the predictions
Xtest = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[2])

# Calculate the Mean Absolute Error loss for each sample
test_mae_loss = np.mean(np.abs(X_pred-Xtest), axis = 1)

# Store the MAE loss in the scored DataFrame
scored['Loss_mae'] = test_mae_loss

# Add the threshold value to the 'scored' DataFrame
scored['Threshold'] = threshold

# Determine if each sample is an anomaly based on the MAE loss and threshold
scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']
scored.head()

# Plotting the Loss Distribution

plt.figure(figsize=(12, 9), dpi=300)  # High DPI for better PDF quality
plt.title('Loss Distribution on the Test Data', fontsize=30)
sns.histplot(scored['Loss_mae'], bins=100, kde=True, color='blue')
plt.xlim([0.0, 0.5])
plt.xlabel('Mean Absolute Error Loss', fontsize=30)
plt.ylabel('Number of Samples', fontsize=30)
plt.xticks(fontsize=30)
plt.yticks(fontsize=30)
plt.tight_layout()  # Adjusts plot to ensure everything fits without overlapping


# Show the plot
plt.show()

scored

# Calculating the precision, recall and F1 scores.

precision_test = precision_score(test_df['at_risk_event'], scored['Anomaly'])
recall_test = recall_score(test_df['at_risk_event'], scored['Anomaly'])
f1_test = f1_score(test_df['at_risk_event'], scored['Anomaly'])

# Printing out the calculated scores.

print(f'Test Precision: {precision_test}')
print(f'Test Recall: {recall_test}')
print(f'Test F1 Score: {f1_test}')

# AUC-ROC Score
auc_roc = roc_auc_score(test_df['at_risk_event'],test_mae_loss)
print(f"AUC-ROC Score: {auc_roc}")

# AUPR Score
precision, recall, _ = precision_recall_curve(test_df['at_risk_event'], test_mae_loss)
aupr = auc(recall, precision)
print(f"AUPR Score: {aupr}")

# Plotting the confusion matrix.

cm = confusion_matrix(test_df['at_risk_event'], scored['Anomaly'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Risk'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for LSTM Autoencoder')
plt.show()